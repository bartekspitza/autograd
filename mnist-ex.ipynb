{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd50b5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tensor import Tensor\n",
    "import src.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mnist import MNIST\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9693099",
   "metadata": {},
   "outputs": [],
   "source": [
    "mndata = MNIST('./mnist_dataset/')\n",
    "mndata.gz = True\n",
    "training, test = mndata.load_training(), mndata.load_testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2952772",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(training[0] + test[0], dtype=float)\n",
    "Y = np.array(list(training[1]) + list(test[1]), dtype=int)\n",
    "\n",
    "# Scale down pixels from 0-255 to 0-1\n",
    "X = X / 255.0\n",
    "\n",
    "# OneHot label vectors\n",
    "one_hot = np.zeros((Y.size, 10)) # (70000, 10)\n",
    "one_hot[np.arange(Y.size), Y] = 1\n",
    "Y = one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50df552d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sizes: train=56000, val=7000, test=7000\n"
     ]
    }
   ],
   "source": [
    "# Split\n",
    "n = len(X)\n",
    "tr, d = int(n*0.8), int(n*0.9)\n",
    "Xtr, Xdev, Xte = Tensor(X[:tr]), Tensor(X[tr:d]), Tensor(X[d:])\n",
    "Ytr, Ydev, Yte = Tensor(Y[:tr]), Tensor(Y[tr:d]), Tensor(Y[d:])\n",
    "print(f'Sizes: train={len(Xtr)}, val={len(Xdev)}, test={len(Xte)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "185d55af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize X\n",
    "img = np.array(Xtr[0].tolist())\n",
    "plt.imshow(img.reshape(28, 28),cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c8ff148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinomial sampling with replacement\n",
    "rng = np.random.default_rng(seed=1)\n",
    "def draw_batch(batch_size): \n",
    "    draw = lambda: int(rng.random() * len(Xtr))\n",
    "    batch = [draw() for _ in range(batch_size)]\n",
    "    return Xtr[batch], Ytr[batch]\n",
    "\n",
    "def accuracy(split):\n",
    "    x, y = {'train': (Xtr, Ytr), 'dev': (Xdev, Ydev), 'test': (Xte, Yte)}[split]\n",
    "    global mlp\n",
    "    x = mlp(x) # (7000, 10)\n",
    "    correct = (x.data.argmax(1) == y.data.argmax(1)).sum()\n",
    "    acc = (correct / len(x)) * 100\n",
    "    print(f'{split}-accuracy: {acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6ad381d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = nn.Sequential((\n",
    "    nn.Linear(784, 784), nn.BatchNorm(784),\n",
    "    nn.Relu(),\n",
    "    nn.Linear(784, 784), nn.BatchNorm(784),\n",
    "    nn.Relu(),\n",
    "    nn.Linear(784, 512), nn.BatchNorm(512),\n",
    "    nn.Relu(),\n",
    "    nn.Linear(512, 256), nn.BatchNorm(256),\n",
    "    nn.Relu(),\n",
    "    nn.Linear(256, 10),\n",
    "    nn.Softmax()\n",
    "))\n",
    "losses = []\n",
    "dev_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29bd7e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: train=0.19189498734801694, val=0.4271812362644015\n",
      "Step 1: train=0.23871917324078828, val=0.42940574129498565\n",
      "Step 2: train=0.06943784719478306, val=0.43073367107005744\n",
      "Step 3: train=0.4381960569849998, val=0.44063551255576766\n",
      "Step 4: train=1.1975220005086642, val=0.4312461078965239\n",
      "Step 5: train=0.1997508926397552, val=0.42895750368658825\n",
      "Step 6: train=0.17589404339181344, val=0.4277060567392156\n",
      "Step 7: train=0.5222461831289289, val=0.4226539897932029\n",
      "Step 8: train=0.19752502693554685, val=0.4306737349137032\n",
      "Step 9: train=0.1828958829996673, val=0.4340309284337929\n",
      "Step 10: train=0.5543107461438485, val=0.4243115074745314\n",
      "Step 11: train=0.5764143736474057, val=0.4256887841649697\n",
      "Step 12: train=0.7438861690452843, val=0.42974508446311616\n",
      "Step 13: train=0.377169956762954, val=0.42869753686089035\n",
      "Step 14: train=0.7157149074430172, val=0.42438165600150307\n",
      "Step 15: train=0.5163576382165512, val=0.43067520115602975\n",
      "Step 16: train=0.15444109501885356, val=0.43064248414726974\n",
      "Step 17: train=0.4618949699064992, val=0.4300201398560082\n",
      "Step 18: train=0.16897374657135755, val=0.43516935040243354\n",
      "Step 19: train=0.1601127074013081, val=0.44168009349524007\n",
      "Step 20: train=0.2959122315509669, val=0.43469857010061697\n",
      "Step 21: train=0.4699192975665001, val=0.4324290989617167\n",
      "Step 22: train=0.23563584388008837, val=0.435414822288792\n",
      "Step 23: train=0.5803419356630521, val=0.4238520277075138\n",
      "Step 24: train=0.7545512333746767, val=0.4284871222255754\n",
      "Step 25: train=0.19650632251874933, val=0.42976858199751145\n",
      "Step 26: train=0.1705565196327179, val=0.42496835544530825\n",
      "Step 27: train=0.5035931678111515, val=0.4206714365438157\n",
      "Step 28: train=0.4778938238616091, val=0.4272730169193617\n",
      "Step 29: train=0.27427675830162473, val=0.4232950566395072\n",
      "Step 30: train=0.2986972551052116, val=0.42310182163959326\n",
      "Step 31: train=0.16364161369946445, val=0.4250348172482534\n",
      "Step 32: train=0.6774433527701638, val=0.42237447334733746\n",
      "Step 33: train=0.27331631908425036, val=0.42517730158286576\n",
      "Step 34: train=0.0731019630116528, val=0.42519924300438366\n",
      "Step 35: train=0.6376668839390067, val=0.43016944311568267\n",
      "Step 36: train=0.08183637459803716, val=0.4287880529138515\n",
      "Step 37: train=0.305306502791449, val=0.42664936414315274\n",
      "Step 38: train=0.20512472851973204, val=0.42190583526183595\n",
      "Step 39: train=0.17581901787664092, val=0.4203568707678251\n",
      "Step 40: train=0.23546422184513008, val=0.4233184696600716\n",
      "Step 41: train=0.11041326846879823, val=0.42162862625256453\n",
      "Step 42: train=0.013151862112967594, val=0.42196763398883586\n",
      "Step 43: train=0.27081319109729884, val=0.42192216881226713\n",
      "Step 44: train=0.06839462654161017, val=0.41993657931867107\n",
      "Step 45: train=0.29228078891777226, val=0.41815888754928554\n",
      "Step 46: train=0.5547846947332377, val=0.41634388644944587\n",
      "Step 47: train=0.3114043662564164, val=0.4246913592931026\n",
      "Step 48: train=0.8448399582801078, val=0.42354845886935494\n",
      "Step 49: train=0.4829505588020511, val=0.4260162569190551\n",
      "Step 50: train=0.2977947589493337, val=0.42555783868854036\n",
      "Step 51: train=0.13113037587696194, val=0.42217385403940705\n",
      "Step 52: train=0.19823144659584577, val=0.4216312732591146\n",
      "Step 53: train=0.7013766697697142, val=0.41542626202442456\n",
      "Step 54: train=0.725118256296794, val=0.416977973330435\n",
      "Step 55: train=0.2836357790936722, val=0.4144937890072066\n",
      "Step 56: train=0.5242991364472898, val=0.418608298029252\n",
      "Step 57: train=0.18726997973649886, val=0.42018709349714906\n",
      "Step 58: train=0.35878263038485536, val=0.4279424037151678\n",
      "Step 59: train=0.4684552857084559, val=0.42056856655215474\n",
      "Step 60: train=0.38907502473803507, val=0.4183333625952472\n",
      "Step 61: train=0.10377097839153589, val=0.4212780705178875\n",
      "Step 62: train=0.09874103989119958, val=0.4221727664957122\n",
      "Step 63: train=0.13143647934313651, val=0.42156771142339444\n",
      "Step 64: train=0.2557641335324087, val=0.428985843455707\n",
      "Step 65: train=0.4927608320072179, val=0.433730609650892\n",
      "Step 66: train=0.25181812724554026, val=0.428632813189977\n",
      "Step 67: train=0.28736823574072395, val=0.4360667457728622\n",
      "Step 68: train=0.24904741902379468, val=0.4282096956467962\n",
      "Step 69: train=0.39827910848443726, val=0.42010380595464225\n",
      "Step 70: train=0.6310181915836381, val=0.42474701679253596\n",
      "Step 71: train=0.24804898725726332, val=0.42316958173670166\n",
      "Step 72: train=0.1466340231040671, val=0.4160654596766105\n",
      "Step 73: train=0.5335569984857695, val=0.41938732490048036\n",
      "Step 74: train=0.2069479796348883, val=0.42264204012565554\n",
      "Step 75: train=0.08562216335647903, val=0.4276447437959485\n",
      "Step 76: train=0.23239443711718033, val=0.43030329369640613\n",
      "Step 77: train=0.31905797112605594, val=0.4310485465311943\n",
      "Step 78: train=0.4925202261286423, val=0.41726725947455556\n",
      "Step 79: train=0.45438973812553823, val=0.42061614346196186\n",
      "Step 80: train=0.048566438092478806, val=0.42444591117603536\n",
      "Step 81: train=0.28093543701103274, val=0.42676833439816203\n",
      "Step 82: train=0.16089178642352925, val=0.4271513574579428\n",
      "Step 83: train=0.39797315131759514, val=0.43537881240346105\n",
      "Step 84: train=0.13129945672753734, val=0.43219572132334155\n",
      "Step 85: train=0.7089486541875036, val=0.433890314149711\n",
      "Step 86: train=0.5942328485749853, val=0.43599594197868025\n",
      "Step 87: train=0.3134539579517256, val=0.44823111280725947\n",
      "Step 88: train=0.2431059203035484, val=0.4261914916721877\n",
      "Step 89: train=0.4123832761817011, val=0.4187482394492107\n",
      "Step 90: train=0.3424683543419603, val=0.42447558916935213\n",
      "Step 91: train=0.4886125194428576, val=0.4282128383782573\n",
      "Step 92: train=0.16891833883448715, val=0.4429583050627567\n",
      "Step 93: train=0.12057781843959955, val=0.43800217462862373\n",
      "Step 94: train=0.2599129307412648, val=0.43336179538046415\n",
      "Step 95: train=0.2320532426764501, val=0.43878326557220276\n",
      "Step 96: train=0.3870624688323703, val=0.4286588698891472\n",
      "Step 97: train=0.6630268829618273, val=0.4457564320349387\n",
      "Step 98: train=0.13112578064813665, val=0.43961453547115836\n",
      "Step 99: train=0.46396794087265575, val=0.43532447389276124\n",
      "Step 100: train=0.2771680196667937, val=0.43539529502429364\n",
      "Step 101: train=0.122559676771151, val=0.43283539801615695\n",
      "Step 102: train=1.0470149263095743, val=0.43279772434963576\n",
      "Step 103: train=0.22903311661742679, val=0.42367060555308783\n",
      "Step 104: train=0.5956211913992895, val=0.4218475656847861\n",
      "Step 105: train=0.1456291615814046, val=0.41963460235843025\n",
      "Step 106: train=0.28761910814189096, val=0.421470091504765\n",
      "Step 107: train=0.3669387908036956, val=0.4174771492667278\n",
      "Step 108: train=0.6246292564458352, val=0.41859687978047777\n",
      "Step 109: train=0.7136505702233683, val=0.4175899290491236\n",
      "Step 110: train=0.5679861627246555, val=0.4167296298865054\n",
      "Step 111: train=0.3364007362050688, val=0.41923734206536184\n",
      "Step 112: train=0.784214584486962, val=0.41578654305458235\n",
      "Step 113: train=0.12510527561584073, val=0.4116751992973599\n",
      "Step 114: train=0.8913934615201178, val=0.41122835269964075\n",
      "Step 115: train=0.47866229422912776, val=0.4092717756015397\n",
      "Step 116: train=0.4863999455459266, val=0.4142611509677481\n",
      "Step 117: train=0.3011503896235034, val=0.41154380536486873\n",
      "Step 118: train=0.5235809657264572, val=0.40913736219947583\n",
      "Step 119: train=0.056168658106166634, val=0.4062016632013519\n",
      "Step 120: train=0.7828040169589174, val=0.41543587078142186\n",
      "Step 121: train=0.42334611581295173, val=0.41460972465136964\n",
      "Step 122: train=0.21892432075672932, val=0.4124466792352875\n",
      "Step 123: train=1.0003687269670338, val=0.4422624500512151\n",
      "Step 124: train=0.0940594162298388, val=0.4486107207128514\n",
      "Step 125: train=0.6337822544524588, val=0.41708757386022255\n",
      "Step 126: train=0.25973413136729756, val=0.41769300907579976\n",
      "Step 127: train=0.2931779130174363, val=0.41580857962022383\n",
      "Step 128: train=0.11186595707102209, val=0.41669593775638764\n",
      "Step 129: train=0.6573270309903749, val=0.4186677363476951\n",
      "Step 130: train=0.484092926813979, val=0.4115242298181128\n",
      "Step 131: train=0.4618783389817945, val=0.4049735473279775\n",
      "Step 132: train=0.29985806159473866, val=0.40808290476535153\n",
      "Step 133: train=0.17751514105482238, val=0.40827359942370145\n",
      "Step 134: train=0.1779235177015388, val=0.4080260669767592\n",
      "Step 135: train=0.37519097739930485, val=0.41019527437734477\n",
      "Step 136: train=0.16336089340491156, val=0.4136822593465989\n",
      "Step 137: train=0.462223064130612, val=0.4063323955592389\n",
      "Step 138: train=0.6966161320049475, val=0.40948063695503273\n",
      "Step 139: train=0.19964516808031074, val=0.4186730471127207\n",
      "Step 140: train=0.45828593263224704, val=0.42672621621111473\n",
      "Step 141: train=0.5796136828992741, val=0.426627408934066\n",
      "Step 142: train=0.1499762849326091, val=0.4168787099499829\n",
      "Step 143: train=0.3245163173121266, val=0.41297427114439755\n",
      "Step 144: train=0.17849191581050472, val=0.4049340423054997\n",
      "Step 145: train=0.4580185554052853, val=0.4019661830153007\n",
      "Step 146: train=0.10009370837907963, val=0.40295053286291005\n",
      "Step 147: train=0.3151986233069806, val=0.4019841272462902\n",
      "Step 148: train=0.5812573876464082, val=0.40406642802055537\n",
      "Step 149: train=0.3667841460665272, val=0.40615536854571255\n",
      "Step 150: train=0.34138013540363393, val=0.4074645140862077\n",
      "Step 151: train=0.5976167919591128, val=0.412073285192757\n",
      "Step 152: train=0.6255783974955338, val=0.4019830248554782\n",
      "Step 153: train=0.34319809576447247, val=0.4046266742647278\n",
      "Step 154: train=0.5673607509693641, val=0.4098701292931051\n",
      "Step 155: train=0.28775836989197356, val=0.40748794562756563\n",
      "Step 156: train=0.2666385240890207, val=0.4057976639207302\n",
      "Step 157: train=0.056330825147072675, val=0.40264959448629706\n",
      "Step 158: train=0.08417442579239989, val=0.40122558617490306\n",
      "Step 159: train=0.5291879593049106, val=0.4039584942722567\n",
      "Step 160: train=0.48301455581607333, val=0.4026082880284812\n",
      "Step 161: train=0.6028236718564979, val=0.41817821702524505\n",
      "Step 162: train=0.2266908218989652, val=0.4158192486753534\n",
      "Step 163: train=0.4582381266553311, val=0.40538111352878425\n",
      "Step 164: train=0.2546539639136395, val=0.40131040348707925\n",
      "Step 165: train=0.28702707446216136, val=0.4030943847503128\n",
      "Step 166: train=0.4986187668098423, val=0.4003281362817987\n",
      "Step 167: train=0.33518749588250013, val=0.4046289329435081\n",
      "Step 168: train=0.36733092164255055, val=0.4064782970209951\n",
      "Step 169: train=0.16914423927946778, val=0.4064194990442891\n",
      "Step 170: train=0.197415955636837, val=0.40727713946900046\n",
      "Step 171: train=0.24057537030445933, val=0.41684470145930114\n",
      "Step 172: train=0.17709598737094945, val=0.41442817648208063\n",
      "Step 173: train=0.6186941780373518, val=0.41228192275743014\n",
      "Step 174: train=0.486698580931057, val=0.4165317355522882\n",
      "Step 175: train=1.087966129423595, val=0.4158398903673462\n",
      "Step 176: train=0.2188126855229578, val=0.4079633580009617\n",
      "Step 177: train=0.39616853451548095, val=0.4023085497751568\n",
      "Step 178: train=0.5724889298086316, val=0.4080389000164227\n",
      "Step 179: train=0.32870491977716326, val=0.40313045405587034\n",
      "Step 180: train=0.5709924672910813, val=0.39959650763814597\n",
      "Step 181: train=0.2745515093651842, val=0.39763949230703693\n",
      "Step 182: train=0.4525314395995423, val=0.394413368965934\n",
      "Step 183: train=0.3509095553486689, val=0.3944830272051878\n",
      "Step 184: train=0.2978402099966783, val=0.39704683488791737\n",
      "Step 185: train=0.5697748761183469, val=0.39672064259601647\n",
      "Step 186: train=0.31391158499907545, val=0.3971094845342242\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/maikzy/gitreps/deeplearning/mnist-ex.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/maikzy/gitreps/deeplearning/mnist-ex.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m out \u001b[39m=\u001b[39m mlp(x)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/maikzy/gitreps/deeplearning/mnist-ex.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m loss \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mnlll(out, y, reduction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/maikzy/gitreps/deeplearning/mnist-ex.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/maikzy/gitreps/deeplearning/mnist-ex.ipynb#X10sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m mlp\u001b[39m.\u001b[39moptimize(lr\u001b[39m=\u001b[39mlr)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/maikzy/gitreps/deeplearning/mnist-ex.ipynb#X10sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m mlp\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/gitreps/deeplearning/src/tensor.py:56\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m \u001b[39mreversed\u001b[39m(topo):\n\u001b[1;32m     55\u001b[0m     \u001b[39mif\u001b[39;00m v\u001b[39m.\u001b[39m_backward \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m         v\u001b[39m.\u001b[39;49m_backward()\n",
      "File \u001b[0;32m~/gitreps/deeplearning/src/tensor.py:202\u001b[0m, in \u001b[0;36mTensor.__matmul__.<locals>.back\u001b[0;34m()\u001b[0m\n\u001b[1;32m    200\u001b[0m grads \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39mrepeat(m, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mreshape((k,m,n))\n\u001b[1;32m    201\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrad \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (x_data\u001b[39m*\u001b[39mgrads)\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m--> 202\u001b[0m x\u001b[39m.\u001b[39mgrad \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mreshape((k,m,\u001b[39m1\u001b[39;49m))\u001b[39m*\u001b[39;49mgrads)\u001b[39m.\u001b[39;49msum(axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/core/_methods.py:46\u001b[0m, in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_amin\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     43\u001b[0m           initial\u001b[39m=\u001b[39m_NoValue, where\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m     44\u001b[0m     \u001b[39mreturn\u001b[39;00m umr_minimum(a, axis, \u001b[39mNone\u001b[39;00m, out, keepdims, initial, where)\n\u001b[0;32m---> 46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sum\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     47\u001b[0m          initial\u001b[39m=\u001b[39m_NoValue, where\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m     48\u001b[0m     \u001b[39mreturn\u001b[39;00m umr_sum(a, axis, dtype, out, keepdims, initial, where)\n\u001b[1;32m     50\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_prod\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     51\u001b[0m           initial\u001b[39m=\u001b[39m_NoValue, where\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "steps = 300\n",
    "batch_size = 32\n",
    "lr = 0.01\n",
    "breakpoint = -1\n",
    "for i in range(steps):\n",
    "    x, y = draw_batch(batch_size)\n",
    "    out = mlp(x)\n",
    "\n",
    "    loss = nn.nlll(out, y, reduction='mean')\n",
    "    loss.backward()\n",
    "\n",
    "    mlp.optimize(lr=lr)\n",
    "    mlp.zero_grad()\n",
    "\n",
    "    loss = loss.data.item()\n",
    "    losses.append(loss)\n",
    "    dev_loss = nn.nlll(mlp(Xdev), Ydev, reduction='mean').data.item()\n",
    "    dev_losses.append(dev_loss)\n",
    "    print(f'Step {i}: train={loss}, val={dev_loss}')\n",
    "\n",
    "plt.plot(losses, label='train')\n",
    "plt.plot(dev_losses, label='dev')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2563c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev-accuracy: 88.54%\n"
     ]
    }
   ],
   "source": [
    "accuracy('dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45bec8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-accuracy: 90.40%\n"
     ]
    }
   ],
   "source": [
    "mlp.inference()\n",
    "accuracy('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a719a1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
